#!/usr/bin/env python3
# run_m3_reproduction.py - M3 ì„±ëŠ¥ ì—­ì „ í˜„ìƒ H100 ê²€ì¦ ì‹¤í—˜

import os
import json
import time
import torch
import argparse
from datetime import datetime
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    AutoConfig,
    GenerationConfig
)
import pandas as pd
from typing import List, Dict, Tuple
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class M3ReproductionExperiment:
    """M3 ì„±ëŠ¥ ì—­ì „ í˜„ìƒì„ H100ì—ì„œ ì¬í˜„í•˜ëŠ” ì‹¤í—˜ í´ë˜ìŠ¤"""
    
    def __init__(self, cache_dir="./models", output_dir="./results"):
        self.cache_dir = cache_dir
        self.output_dir = output_dir
        if torch.cuda.is_available():
            self.device = torch.device(f"cuda:{torch.cuda.current_device()}")
            self.gpu_id = torch.cuda.current_device()
        else:
            self.device = torch.device("cpu")
            self.gpu_id = None

        self.models = {}
        self.tokenizers = {}
        self.results = []
        
        # H100 ì „ìš© ì„¤ì •
        self.h100_config = {
            "max_memory_per_gpu": "15GB",  # ëª¨ë¸ë‹¹ ìµœëŒ€ ë©”ëª¨ë¦¬
            "batch_size": 2,                 
            "max_new_tokens": 512,           # ê¸´ ìƒì„± í…ŒìŠ¤íŠ¸
            "temperature": 0.7,
            "top_p": 0.9,
            "do_sample": True
        }
        
        # M3ì—ì„œ ë‹¤ìš´ë¡œë“œëœ ëª¨ë¸ë“¤
        self.available_models = {
            # ê¸°ë³¸ M3 ëª¨ë¸ë“¤
            "117M": "microsoft/DialoGPT-small",
            "1.5B": "Qwen/Qwen2-1.5B-Instruct", 
            "3.8B": "microsoft/Phi-3.5-mini-instruct",
            "7B": "Qwen/Qwen2-7B-Instruct",
            "7B_alt": "microsoft/Phi-3-small-8k-instruct",
    
            # í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸ë“¤
            "beomi/llama-2-ko-7b": "beomi/llama-2-ko-7b",
            "kakaobrain/kogpt": "kakaobrain/kogpt", 
            "nlpai-lab/kullm-polyglot-12.8b-v2": "nlpai-lab/kullm-polyglot-12.8b-v2",
            "upstage/SOLAR-10.7B-Instruct-v1.0": "upstage/SOLAR-10.7B-Instruct-v1.0",
    
            # í¬ê¸°ë³„ ë³„ì¹­ (í¸ì˜ìš©)
            "ko-7b": "beomi/llama-2-ko-7b",
            "kogpt": "kakaobrain/kogpt",
            "kullm": "nlpai-lab/kullm-polyglot-12.8b-v2",
            "solar": "upstage/SOLAR-10.7B-Instruct-v1.0"
        }
        
        os.makedirs(output_dir, exist_ok=True)
    
    def load_model(self, model_size: str) -> Tuple[AutoTokenizer, AutoModelForCausalLM]:
        """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ"""
        if model_size not in self.available_models:
            raise ValueError(f"Model size {model_size} not available. Available: {list(self.available_models.keys())}")
    
        model_name = self.available_models[model_size]
    
        if model_size in self.models:
            logger.info(f"â™»ï¸  Reusing cached model: {model_name}")
            return self.tokenizers[model_size], self.models[model_size]
    
        logger.info(f"ğŸ“¥ Loading model: {model_name} ({model_size})")
        start_time = time.time()
    
        try:
            # KoGPT íŠ¹ë³„ ì²˜ë¦¬
            if model_name == "kakaobrain/kogpt":
                return self.load_kogpt_model(model_size)
        
            # KULLM íŠ¹ë³„ ì²˜ë¦¬
            if model_name == "nlpai-lab/kullm-polyglot-12.8b-v2":
                return self.load_kullm_model(model_size)
        
            # ì¼ë°˜ ëª¨ë¸ ë¡œë“œ
            return self.load_regular_model(model_size, model_name)
        
        except Exception as e:
            logger.error(f"âŒ Failed to load {model_name}: {e}")
            raise

    def load_regular_model(self, model_size: str, model_name: str) -> Tuple[AutoTokenizer, AutoModelForCausalLM]:
        """ì¼ë°˜ ëª¨ë¸ ë¡œë”©"""
        start_time = time.time()
    
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained(
            model_name, 
            cache_dir=self.cache_dir,
            trust_remote_code=True  
        )
    
        # pad_token ì„¤ì •
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
    
        # ëª¨ë¸ ë¡œë“œ (H100ì— ìµœì í™”)
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            cache_dir=self.cache_dir,
            torch_dtype=torch.float16,  # H100ì—ì„œ ë¹ ë¥¸ ì—°ì‚°
            device_map="auto",          # ìë™ GPU ë°°ì¹˜
            trust_remote_code=True,
            max_memory={self.gpu_id: self.h100_config["max_memory_per_gpu"]} if self.gpu_id is not None else None
        )
    
        # ìƒì„± ì„¤ì •
        model.generation_config = GenerationConfig(
            max_new_tokens=self.h100_config["max_new_tokens"],
            temperature=self.h100_config["temperature"],
            top_p=self.h100_config["top_p"],
            do_sample=self.h100_config["do_sample"],
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id
        )
    
        load_time = time.time() - start_time
    
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
        if torch.cuda.is_available():
            memory_used = torch.cuda.max_memory_allocated() / 1e9
            logger.info(f"âœ… Model loaded in {load_time:.1f}s, GPU Memory: {memory_used:.1f}GB")
    
        # ìºì‹œì— ì €ì¥
        self.tokenizers[model_size] = tokenizer
        self.models[model_size] = model
    
        return tokenizer, model

    def load_kogpt_model(self, model_size: str) -> Tuple[AutoTokenizer, AutoModelForCausalLM]:
        """KoGPT ì „ìš© ë¡œë”© í•¨ìˆ˜"""
        start_time = time.time()
    
        # KoGPT ì „ìš© í† í¬ë‚˜ì´ì € ì„¤ì •
        tokenizer = AutoTokenizer.from_pretrained(
            'kakaobrain/kogpt',
            revision='KoGPT6B-ryan1.5b-float16',  # ì¤‘ìš”: revision ì§€ì •
            bos_token='[BOS]',
            eos_token='[EOS]',
            unk_token='[UNK]',
            pad_token='[PAD]',
            mask_token='[MASK]',
            cache_dir=self.cache_dir,
            trust_remote_code=True
        )
    
        # KoGPT ì „ìš© ëª¨ë¸ ë¡œë“œ
        model = AutoModelForCausalLM.from_pretrained(
            'kakaobrain/kogpt',
            revision='KoGPT6B-ryan1.5b-float16',  # ì¤‘ìš”: revision ì§€ì •
            pad_token_id=tokenizer.eos_token_id,
            torch_dtype=torch.float16,
            low_cpu_mem_usage=True,
            device_map="auto",
            cache_dir=self.cache_dir,
            trust_remote_code=True
        )
    
        # ìƒì„± ì„¤ì •
        model.generation_config = GenerationConfig(
            max_new_tokens=self.h100_config["max_new_tokens"],
            temperature=self.h100_config["temperature"],
            top_p=self.h100_config["top_p"],
            do_sample=self.h100_config["do_sample"],
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id
        )
    
        load_time = time.time() - start_time
    
        if torch.cuda.is_available():
            memory_used = torch.cuda.max_memory_allocated() / 1e9
        logger.info(f"âœ… KoGPT loaded in {load_time:.1f}s, GPU Memory: {memory_used:.1f}GB")
    
        # ìºì‹œì— ì €ì¥
        self.tokenizers[model_size] = tokenizer
        self.models[model_size] = model
    
        return tokenizer, model

    def load_kullm_model(self, model_size: str) -> Tuple[AutoTokenizer, AutoModelForCausalLM]:
        """KULLM ì „ìš© ë¡œë”© í•¨ìˆ˜ (ì˜¤í”„ë¡œë“œ ì²˜ë¦¬)"""
        start_time = time.time()
    
        # ì˜¤í”„ë¡œë“œ ë””ë ‰í† ë¦¬ ìƒì„±
        offload_dir = os.path.join(self.cache_dir, "offload")
        os.makedirs(offload_dir, exist_ok=True)
    
        # KULLM í† í¬ë‚˜ì´ì €
        tokenizer = AutoTokenizer.from_pretrained(
            'nlpai-lab/kullm-polyglot-12.8b-v2',
            cache_dir=self.cache_dir,
            trust_remote_code=True
        )
    
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
    
        # KULLM ëª¨ë¸ (ì˜¤í”„ë¡œë“œ ì„¤ì •)
        model = AutoModelForCausalLM.from_pretrained(
            'nlpai-lab/kullm-polyglot-12.8b-v2',
            torch_dtype=torch.float16,
            device_map="auto",
            offload_folder=offload_dir,  # ì¤‘ìš”: ì˜¤í”„ë¡œë“œ í´ë” ì§€ì •
            low_cpu_mem_usage=True,
            cache_dir=self.cache_dir,
            trust_remote_code=True
        )
    
        # ìƒì„± ì„¤ì •
        model.generation_config = GenerationConfig(
            max_new_tokens=self.h100_config["max_new_tokens"],
            temperature=self.h100_config["temperature"],
            top_p=self.h100_config["top_p"],
            do_sample=self.h100_config["do_sample"],
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id
        )
    
        load_time = time.time() - start_time
    
        if torch.cuda.is_available():
            memory_used = torch.cuda.max_memory_allocated() / 1e9
            logger.info(f"âœ… KULLM loaded in {load_time:.1f}s, GPU Memory: {memory_used:.1f}GB")
    
        # ìºì‹œì— ì €ì¥
        self.tokenizers[model_size] = tokenizer
        self.models[model_size] = model
    
        return tokenizer, model
    
    def generate_text(self, model_size: str, prompts: List[str]) -> List[Dict]:
        """í…ìŠ¤íŠ¸ ìƒì„± ë° ì„±ëŠ¥ ì¸¡ì •"""
        tokenizer, model = self.load_model(model_size)
        
        results = []
        batch_size = self.h100_config["batch_size"]
        
        logger.info(f"ğŸš€ Generating with {model_size} model, batch_size={batch_size}")
        
        for i in range(0, len(prompts), batch_size):
            batch_prompts = prompts[i:i+batch_size]
            
            # í† í¬ë‚˜ì´ì§• (KULLM í˜¸í™˜ì„± ê°œì„ )
            if model_size == "kullm":
                inputs = tokenizer(
                    batch_prompts, 
                    return_tensors="pt", 
                    padding=True, 
                    truncation=True,
                    max_length=512,
                    return_token_type_ids=False  # KULLMì€ token_type_ids ë¶ˆí•„ìš”
                ).to(self.device)
            else:
                inputs = tokenizer(
                    batch_prompts, 
                    return_tensors="pt", 
                    padding=True, 
                    truncation=True,
                    max_length=512
                ).to(self.device)
            
            # ìƒì„± ì‹œì‘
            start_time = time.time()
            torch.cuda.synchronize()  # ì •í™•í•œ ì‹œê°„ ì¸¡ì •
            
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=self.h100_config["max_new_tokens"],
                    temperature=self.h100_config["temperature"],
                    top_p=self.h100_config["top_p"],
                    do_sample=self.h100_config["do_sample"],
                    pad_token_id=tokenizer.pad_token_id,
                    eos_token_id=tokenizer.eos_token_id
                )
            
            torch.cuda.synchronize()  # GPU ì‘ì—… ì™„ë£Œ ëŒ€ê¸°
            generation_time = time.time() - start_time
            
            # ë””ì½”ë”©
            generated_texts = []
            for j, output in enumerate(outputs):
                # ì…ë ¥ ì œê±°í•˜ê³  ìƒˆë¡œ ìƒì„±ëœ ë¶€ë¶„ë§Œ
                input_length = inputs['input_ids'][j].shape[0]
                generated = output[input_length:]
                generated_text = tokenizer.decode(generated, skip_special_tokens=True)
                generated_texts.append(generated_text)
            
            # ê²°ê³¼ ì €ì¥
            for j, (prompt, generated) in enumerate(zip(batch_prompts, generated_texts)):
                results.append({
                    "model_size": model_size,
                    "model_name": self.available_models[model_size],
                    "prompt": prompt,
                    "generated": generated,
                    "generation_time": generation_time / len(batch_prompts),  # í‰ê·  ì‹œê°„
                    "tokens_generated": len(tokenizer.encode(generated)),
                    "tokens_per_second": len(tokenizer.encode(generated)) / (generation_time / len(batch_prompts)),
                    "timestamp": datetime.now().isoformat()
                })
        
        logger.info(f"âœ… Generated {len(results)} responses with {model_size}")
        return results
    
    def run_korean_benchmark(self, model_sizes: List[str] = None):
        """í•œêµ­ì–´ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        if model_sizes is None:
            model_sizes = ["117M", "1.5B", "3.8B", "7B"]
        
        # í•œêµ­ì–´ í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë“¤
        korean_prompts = [
            "í•œêµ­ì˜ ì „í†µ ìŒì‹ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
            "AI ê¸°ìˆ ì˜ ë¯¸ë˜ì— ëŒ€í•œ ë‹¹ì‹ ì˜ ìƒê°ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            "ê¸°í›„ ë³€í™” ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ë°©ë²•ì„ ì œì‹œí•´ì£¼ì„¸ìš”.",
            "í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ì˜ ì–´ë ¤ìš´ ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            "ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì˜ ì°¨ì´ì ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
            "í•œêµ­ ë¬¸í™”ê°€ ì„¸ê³„ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì— ëŒ€í•´ ë…¼í•´ë³´ì„¸ìš”.",
            "íš¨ìœ¨ì ì¸ í•™ìŠµ ë°©ë²•ì— ëŒ€í•œ ì¡°ì–¸ì„ í•´ì£¼ì„¸ìš”.",
            "ì¸ê³µì§€ëŠ¥ê³¼ ì¸ê°„ì˜ í˜‘ë ¥ ê´€ê³„ëŠ” ì–´ë–»ê²Œ ë°œì „í• ê¹Œìš”?"
        ]
        
        logger.info(f"ğŸ‡°ğŸ‡· Starting Korean benchmark with {len(korean_prompts)} prompts")
        logger.info(f"ğŸ“Š Testing model sizes: {model_sizes}")
        
        all_results = []
        
        for model_size in model_sizes:
            logger.info(f"\n{'='*50}")
            logger.info(f"Testing {model_size} model")
            logger.info(f"{'='*50}")
            
            try:
                results = self.generate_text(model_size, korean_prompts)
                all_results.extend(results)
                
                # ì¤‘ê°„ ê²°ê³¼ ì €ì¥
                self.save_results(all_results, f"korean_benchmark_partial_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
                
            except Exception as e:
                logger.error(f"âŒ Error testing {model_size}: {e}")
                continue
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        
        # ìµœì¢… ê²°ê³¼ ì €ì¥
        final_file = f"korean_benchmark_final_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        self.save_results(all_results, final_file)
        
        # ì„±ëŠ¥ ë¶„ì„
        self.analyze_results(all_results)
        
        return all_results
    
    def run_size_comparison(self):
        """M3 í¬ê¸°ë³„ ì„±ëŠ¥ ë¹„êµ ì¬í˜„"""
        logger.info("ğŸ”¬ M3 Size Comparison Reproduction on H100")
        
        # M3 ì›ë…¼ë¬¸ ìŠ¤íƒ€ì¼ í”„ë¡¬í”„íŠ¸
        comparison_prompts = [
            "Translate the following English sentence to Korean: 'Artificial intelligence is transforming our world.'",
            "í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”: ì¸ê³µì§€ëŠ¥ì˜ ì¥ì ê³¼ ë‹¨ì ì„ ë¹„êµí•´ë³´ì„¸ìš”.",
            "Solve this problem step by step: If a model has 1.5 billion parameters and another has 3.8 billion, which would you expect to perform better?",
            "í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”? ê·¸ë¦¬ê³  ê·¸ ë„ì‹œì˜ íŠ¹ì§•ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
            "Write a short story about a small model outperforming a larger model.",
            "ë‹¤ìŒ ìˆ˜í•™ ë¬¸ì œë¥¼ í•´ê²°í•˜ì„¸ìš”: 2^10 + 3^4 = ?",
            "Explain the concept of 'emergence' in language models.",
            "í•œêµ­ì–´ ì²˜ë¦¬ì—ì„œ í† í¬ë‚˜ì´ì œì´ì…˜ì˜ ì¤‘ìš”ì„±ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”."
        ]
        
        # M3 í•µì‹¬ ë¹„êµ: 1.5B vs 3.8B
        key_sizes = ["1.5B", "3.8B"]
        logger.info(f"ğŸ¯ Key M3 comparison: {key_sizes}")
        
        results = []
        for size in key_sizes:
            size_results = self.generate_text(size, comparison_prompts)
            results.extend(size_results)
        
        # ì¶”ê°€ í¬ê¸°ë“¤ë„ í…ŒìŠ¤íŠ¸
        additional_sizes = ["117M", "7B"]
        for size in additional_sizes:
            if size in self.available_models:
                size_results = self.generate_text(size, comparison_prompts)
                results.extend(size_results)
        
        # M3 ìŠ¤íƒ€ì¼ ë¶„ì„
        self.analyze_m3_phenomenon(results)
        
        return results
    
    def analyze_results(self, results: List[Dict]):
        """ê²°ê³¼ ë¶„ì„ ë° ì‹œê°í™”"""
        df = pd.DataFrame(results)
        
        if df.empty:
            logger.warning("âš ï¸  No results to analyze")
            return
        
        logger.info("\nğŸ“Š Performance Analysis")
        logger.info("=" * 50)
        
        # ëª¨ë¸ë³„ ì„±ëŠ¥ ìš”ì•½
        summary = df.groupby(['model_size', 'model_name']).agg({
            'generation_time': ['mean', 'std'],
            'tokens_per_second': ['mean', 'std'],
            'tokens_generated': ['mean', 'std']
        }).round(3)
        
        print("\nğŸ“ˆ Performance Summary:")
        print(summary)
        
        # ê°€ì¥ ë¹ ë¥¸ ëª¨ë¸
        fastest_model = df.loc[df['tokens_per_second'].idxmax()]
        logger.info(f"\nğŸš€ Fastest model: {fastest_model['model_size']} ({fastest_model['tokens_per_second']:.1f} tokens/sec)")
        
        # ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥
        csv_file = os.path.join(self.output_dir, f"performance_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv")
        summary.to_csv(csv_file)
        logger.info(f"ğŸ’¾ Summary saved to: {csv_file}")
    
    def analyze_m3_phenomenon(self, results: List[Dict]):
        """M3 í˜„ìƒ ë¶„ì„"""
        df = pd.DataFrame(results)
        
        if df.empty:
            return
        
        logger.info("\nğŸ”¬ M3 Phenomenon Analysis")
        logger.info("=" * 50)
        
        # 1.5B vs 3.8B ì§ì ‘ ë¹„êµ
        df_1_5b = df[df['model_size'] == '1.5B']
        df_3_8b = df[df['model_size'] == '3.8B']
        
        if not df_1_5b.empty and not df_3_8b.empty:
            avg_speed_1_5b = df_1_5b['tokens_per_second'].mean()
            avg_speed_3_8b = df_3_8b['tokens_per_second'].mean()
            
            logger.info(f"ğŸ“Š 1.5B average speed: {avg_speed_1_5b:.1f} tokens/sec")
            logger.info(f"ğŸ“Š 3.8B average speed: {avg_speed_3_8b:.1f} tokens/sec")
            
            if avg_speed_1_5b > avg_speed_3_8b:
                speed_advantage = ((avg_speed_1_5b - avg_speed_3_8b) / avg_speed_3_8b) * 100
                logger.info(f"ğŸ¯ M3 PHENOMENON CONFIRMED: 1.5B is {speed_advantage:.1f}% faster than 3.8B!")
            else:
                logger.info("ğŸ¤” No clear M3 phenomenon observed in speed")
        
        # ì§ˆì  ë¶„ì„ì„ ìœ„í•œ ìƒ˜í”Œ ì¶œë ¥
        logger.info("\nğŸ“ Sample outputs for quality comparison:")
        for size in ['1.5B', '3.8B']:
            if not df[df['model_size'] == size].empty:
                sample = df[df['model_size'] == size].iloc[0]
                logger.info(f"\n{size} Model Sample:")
                logger.info(f"Prompt: {sample['prompt'][:100]}...")
                logger.info(f"Generated: {sample['generated'][:200]}...")
    
    def save_results(self, results: List[Dict], filename: str):
        """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        filepath = os.path.join(self.output_dir, filename)
        
        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
        output_data = {
            "experiment_info": {
                "timestamp": datetime.now().isoformat(),
                "device": str(self.device),
                "gpu_name": torch.cuda.get_device_name() if torch.cuda.is_available() else "CPU",
                "total_gpu_memory": f"{torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB" if torch.cuda.is_available() else "N/A",
                "model_cache_dir": self.cache_dir,
                "h100_config": self.h100_config
            },
            "results": results
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ’¾ Results saved to: {filepath}")

def main():
    parser = argparse.ArgumentParser(description="M3 Reproduction Experiment on H100")
    parser.add_argument("--experiment", choices=["korean", "m3", "both"], default="both",
                       help="Experiment type to run")
    parser.add_argument("--models", nargs="+", default=["117M", "1.5B", "3.8B", "7B"],
                       help="Model sizes to test")
    parser.add_argument("--cache-dir", default="./models", help="Model cache directory")
    parser.add_argument("--output-dir", default="./results", help="Output directory")
    parser.add_argument("--batch-size", type=int, default=8, help="Batch size for generation")
    
    args = parser.parse_args()
    
    # ì‹¤í—˜ ì´ˆê¸°í™”
    experiment = M3ReproductionExperiment(
        cache_dir=args.cache_dir,
        output_dir=args.output_dir
    )
    
    if args.batch_size:
        experiment.h100_config["batch_size"] = args.batch_size
    
    logger.info("ğŸš€ Starting M3 Reproduction Experiment on H100")
    logger.info(f"ğŸ“Š Models to test: {args.models}")
    logger.info(f"ğŸ¯ Experiment type: {args.experiment}")
    
    # GPU ì •ë³´ ì¶œë ¥
    if torch.cuda.is_available():
        logger.info(f"ğŸ”¥ GPU: {torch.cuda.get_device_name()}")
        logger.info(f"ğŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
    
    try:
        if args.experiment in ["korean", "both"]:
            logger.info("\nğŸ‡°ğŸ‡· Starting Korean Benchmark...")
            experiment.run_korean_benchmark(args.models)
        
        if args.experiment in ["m3", "both"]:
            logger.info("\nğŸ”¬ Starting M3 Size Comparison...")
            experiment.run_size_comparison()
        
        logger.info("\nğŸ‰ Experiment completed successfully!")
        
    except Exception as e:
        logger.error(f"âŒ Experiment failed: {e}")
        raise

if __name__ == "__main__":
    main()